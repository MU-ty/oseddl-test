"""
å¢å¼ºå‹æ•°æ®è§£æå™¨ - è§„åˆ™ + LLM æ··åˆæå–
ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´ã€åœ°ç‚¹ã€é“¾æ¥ç­‰ç»“æ„åŒ–æ•°æ®
ç„¶åä½¿ç”¨ LLM è¡¥å……æè¿°ã€æ ‡ç­¾ç­‰éç»“æ„åŒ–æ•°æ®
"""

import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum

class ActivityCategory(str, Enum):
    """æ´»åŠ¨åˆ†ç±»"""
    CONFERENCE = "conference"
    COMPETITION = "competition"
    ACTIVITY = "activity"

@dataclass
class TimelineEvent:
    deadline: str
    comment: str
    
    def to_dict(self) -> Dict:
        return {"deadline": self.deadline, "comment": self.comment}

@dataclass
class ActivityEvent:
    year: int
    id: str
    link: str
    timeline: List[TimelineEvent] = field(default_factory=list)
    timezone: str = "Asia/Shanghai"
    date: str = ""
    place: str = ""
    
    def to_dict(self) -> Dict:
        return {
            "year": self.year,
            "id": self.id,
            "link": self.link,
            "timeline": [t.to_dict() for t in self.timeline],
            "timezone": self.timezone,
            "date": self.date,
            "place": self.place
        }

@dataclass
class ParsedActivity:
    title: str
    description: str
    category: Union[ActivityCategory, str]
    tags: List[str] = field(default_factory=list)
    events: List[ActivityEvent] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            "title": self.title,
            "description": self.description,
            "category": self.category.value if isinstance(self.category, ActivityCategory) else self.category,
            "tags": self.tags,
            "events": [e.to_dict() for e in self.events]
        }
    
    def to_yaml_str(self) -> str:
        """è½¬æ¢ä¸ºYAMLæ ¼å¼å­—ç¬¦ä¸²"""
        try:
            import yaml
            data = self.to_dict()
            return yaml.dump([data], allow_unicode=True, sort_keys=False, default_flow_style=False)
        except:
            import json
            return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)

class EnhancedDataParser:
    """å¢å¼ºçš„æ•°æ®è§£æå™¨ - è§„åˆ™ + LLM"""
    
    def __init__(self):
        import logging
        logger = logging.getLogger(__name__)
        
        self.llm = None
        try:
            from github_models_parser import GitHubModelsParser
            from config import settings
            
            if settings.GITHUB_TOKEN:
                self.llm = GitHubModelsParser(settings.GITHUB_TOKEN, model="gpt-4o")
                logger.info(f"âœ… GitHub Models å·²å¯ç”¨ (Token: {settings.GITHUB_TOKEN[:10]}...)")
            else:
                logger.warning("âš ï¸ GITHUB_TOKEN æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨çº¯è§„åˆ™æå–ï¼ˆæ— LLMï¼‰")
        except Exception as e:
            logger.error(f"âŒ GitHub Models åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def extract_time_info(self, text: str) -> Tuple[Optional[str], List[TimelineEvent]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´ä¿¡æ¯ï¼Œè¿”å›æ—¥æœŸå’Œæ—¶é—´çº¿äº‹ä»¶"""
        import logging
        logger = logging.getLogger(__name__)
        
        timeline = []
        date_str = None
        
        # ä¼˜å…ˆçº§ 1: å®Œæ•´æ—¶é—´æ®µ "2025å¹´11æœˆ1æ—¥ï¼ˆæ˜ŸæœŸå…­ï¼‰09:00-18:00"
        time_range_patterns = [
            # æ ¼å¼: 2025å¹´11æœˆ1æ—¥ï¼ˆæ˜ŸæœŸå…­ï¼‰09:00-18:00 æˆ– 2025å¹´7æœˆ18æ—¥ 09:00-18:00
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:[ï¼ˆ(].*?[ï¼‰)])?\s*(\d{1,2}):(\d{2})\s*[-~è‡³åˆ°]\s*(\d{1,2}):(\d{2})',
            # æ ¼å¼: 2025-11-01 09:00-18:00
            r'(\d{4})-(\d{1,2})-(\d{1,2})[T\s]+(\d{1,2}):(\d{2})\s*[-~è‡³åˆ°]\s*(\d{1,2}):(\d{2})',
        ]
        
        for pattern in time_range_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    year, month, day, h1, m1, h2, m2 = [int(g) for g in match.groups()]
                    date_str = f"{year}-{month:02d}-{day:02d}"
                    
                    timeline.append(TimelineEvent(
                        deadline=f"{year}-{month:02d}-{day:02d}T{h1:02d}:{m1:02d}:00",
                        comment='æ´»åŠ¨å¼€å§‹'
                    ))
                    timeline.append(TimelineEvent(
                        deadline=f"{year}-{month:02d}-{day:02d}T{h2:02d}:{m2:02d}:00",
                        comment='æ´»åŠ¨ç»“æŸ'
                    ))
                    
                    logger.info(f"âœ“ æå–åˆ°æ—¶é—´æ®µ: {date_str} {h1:02d}:{m1:02d}-{h2:02d}:{m2:02d}")
                    return date_str, timeline
                except Exception as e:
                    logger.warning(f"âš ï¸ æ—¶é—´æ®µè§£æå¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§ 2: ISO 8601 æ ¼å¼æ—¶é—´èŒƒå›´
        iso_range_pattern = r'(\d{4})-(\d{1,2})-(\d{1,2})T(\d{1,2}):(\d{2}):(\d{2})\s*[-~]\s*(\d{4})-(\d{1,2})-(\d{1,2})T(\d{1,2}):(\d{2}):(\d{2})'
        iso_match = re.search(iso_range_pattern, text)
        if iso_match:
            try:
                s_year, s_month, s_day, s_hour, s_min, s_sec, e_year, e_month, e_day, e_hour, e_min, e_sec = \
                    [int(g) for g in iso_match.groups()]
                
                date_str = f"{s_year}-{s_month:02d}-{s_day:02d}"
                
                timeline = [
                    TimelineEvent(
                        deadline=f"{s_year}-{s_month:02d}-{s_day:02d}T{s_hour:02d}:{s_min:02d}:{s_sec:02d}",
                        comment='æ´»åŠ¨å¼€å§‹'
                    ),
                    TimelineEvent(
                        deadline=f"{e_year}-{e_month:02d}-{e_day:02d}T{e_hour:02d}:{e_min:02d}:{e_sec:02d}",
                        comment='æ´»åŠ¨ç»“æŸ'
                    )
                ]
                return date_str, timeline
            except:
                pass
        
        # ä¼˜å…ˆçº§ 3: åˆ†åˆ«çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
        start_pattern = r'(?:å¼€å§‹|start)[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[ï¼Œ,\s]+(\d{1,2}):(\d{2})'
        end_pattern = r'(?:ç»“æŸ|end)[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[ï¼Œ,\s]+(\d{1,2}):(\d{2})'
        
        start_match = re.search(start_pattern, text)
        end_match = re.search(end_pattern, text)
        
        if start_match and end_match:
            try:
                s_year, s_month, s_day, s_hour, s_min = [int(g) for g in start_match.groups()]
                e_year, e_month, e_day, e_hour, e_min = [int(g) for g in end_match.groups()]
                
                date_str = f"{s_year}-{s_month:02d}-{s_day:02d}"
                
                timeline = [
                    TimelineEvent(
                        deadline=f"{s_year}-{s_month:02d}-{s_day:02d}T{s_hour:02d}:{s_min:02d}:00",
                        comment='æ´»åŠ¨å¼€å§‹'
                    ),
                    TimelineEvent(
                        deadline=f"{e_year}-{e_month:02d}-{e_day:02d}T{e_hour:02d}:{e_min:02d}:00",
                        comment='æ´»åŠ¨ç»“æŸ'
                    )
                ]
                return date_str, timeline
            except:
                pass
        
        # ä¼˜å…ˆçº§ 4: åªæœ‰æ—¥æœŸ "YYYYå¹´MMæœˆDDæ—¥"
        if not timeline:
            single_time_patterns = [
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?![0-9:])',
                r'(\d{4})-(\d{1,2})-(\d{1,2})(?![T0-9:])',
                r'time[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})',
            ]
            
            for pattern in single_time_patterns:
                match = re.search(pattern, text)
                if match:
                    try:
                        year, month, day = [int(g) for g in match.groups()[:3]]
                        date_str = f"{year}-{month:02d}-{day:02d}"
                        timeline.append(TimelineEvent(
                            deadline=f"{date_str}T00:00:00",
                            comment='å…³é”®æ—¥æœŸ'
                        ))
                        return date_str, timeline
                    except:
                        pass
        
        return date_str, timeline
    
    def extract_place_info(self, text: str) -> Optional[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åœ°ç‚¹ä¿¡æ¯ï¼Œå¹¶æ¸…ç†æ— å…³ä¿¡æ¯"""
        
        patterns = [
            r'(?:åœ°ç‚¹|åœ°å€|ä¸¾åŠåœ°ç‚¹|ä¸¾åŠåœ°)[ï¼š:]\s*([^\nã€‚ï¼Œï¼›ï¼›\|]+)',
            r'(?:Location|Place)[ï¼š:]\s*([^\nã€‚ï¼Œï¼›ï¼›\|]+)',
            r'ğŸ“\s*([^\nã€‚ï¼Œï¼›ï¼›\|]+)',
        ]
        
        place = None
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                place = match.group(1).strip()
                break
        
        if not place:
            return None
        
        # æ¸…ç†æ— å…³ä¿¡æ¯
        remove_keywords = [
            r'æ¨è.*?(?=\s*[ï¼Œï¼›ï¼›]|$)',  # æ¨èåœè½¦ä½ç­‰
            r'[ï¼Œï¼›ï¼›]\s*(?:åœè½¦|åœ°é“|å…¬äº¤|åœ°é“çº¿è·¯|å…¬äº¤è½¦|è·ç¦»|é™„è¿‘|æ¨è|æ­¥è¡Œ|å¼€è½¦|ä¹˜å).*?(?=\s*[ï¼Œï¼›ï¼›]|$)',
            r'[ï¼Œï¼›ï¼›]\s*\d+å…ƒ/å°æ—¶.*?(?=\s*[ï¼Œï¼›ï¼›]|$)',
            r'[ï¼Œï¼›ï¼›]\s*\d+(?:å·çº¿|è·¯|ç±³).*?(?=\s*[ï¼Œï¼›ï¼›]|$)',
            r'ç‚¹å‡»æŠ¥å.*?$',
            r'é•¿æŒ‰.*?$',
            r'æ‰«æ.*?$',
        ]
        
        for pattern in remove_keywords:
            place = re.sub(pattern, '', place, flags=re.IGNORECASE)
        
        place = place.strip()
        place = re.sub(r'[ï¼Œï¼›ï¼›]$', '', place)
        
        # é™åˆ¶é•¿åº¦å¹¶éªŒè¯
        if place and len(place) > 3:
            place = place[:80]
            if re.search(r'[\u4e00-\u9fa5a-zA-Z]+', place):
                return place
        
        return None
    
    def extract_description(self, text: str) -> str:
        """æå–æ´»åŠ¨æè¿°"""
        
        # å–å‰ 200 ä¸ªå­—ç¬¦ä½œä¸ºæè¿°
        lines = text.split('\n')
        description = ''
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('æ—¶é—´') and not line.startswith('åœ°ç‚¹'):
                description += line + ' '
                if len(description) > 200:
                    break
        
        return description[:300] if description else 'æ´»åŠ¨ä¿¡æ¯'
    
    def extract_tags(self, title: str, text: str) -> List[str]:
        """è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾"""
        
        tags = []
        
        # åŸºäºæ ‡é¢˜å’Œå†…å®¹çš„å…³é”®è¯
        keywords = {
            'å¼€æº': ['å¼€æº', 'open source', 'opensource'],
            'æ ¡å›­': ['å¤§å­¦', 'é«˜æ ¡', 'æ ¡å›­', 'university', 'campus'],
            'ä¼šè®®': ['ä¼šè®®', 'conference', 'summit'],
            'ç«èµ›': ['ç«èµ›', 'competition', 'æ¯”èµ›', 'contest'],
            'è®²åº§': ['è®²åº§', 'talk', 'seminar'],
            'å·¥ä½œåŠ': ['å·¥ä½œåŠ', 'workshop', 'ç ”è®¨'],
        }
        
        combined_text = (title + ' ' + text).lower()
        
        for tag, keywords_list in keywords.items():
            for keyword in keywords_list:
                if keyword.lower() in combined_text:
                    tags.append(tag)
                    break
        
        return list(set(tags))[:5]  # æœ€å¤š 5 ä¸ªæ ‡ç­¾
    
    async def parse(self, extracted_text: str, source_url: str = None) -> ParsedActivity:
        """
        è§£ææå–çš„æ–‡æœ¬ - æ··åˆç­–ç•¥ï¼šè§„åˆ™ä¼˜å…ˆ + LLMè¡¥å……
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # ç¬¬ 1 æ­¥ï¼šä½¿ç”¨è§„åˆ™æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆæ›´å¯é ï¼‰
        logger.info("ğŸ“‹ æ­¥éª¤1: ä½¿ç”¨è§„åˆ™æå–ç»“æ„åŒ–ä¿¡æ¯...")
        date_str, timeline = self.extract_time_info(extracted_text)
        place = self.extract_place_info(extracted_text)
        
        logger.info(f"  - æ—¥æœŸ: {date_str or 'æœªæå–åˆ°'}")
        logger.info(f"  - åœ°ç‚¹: {place or 'æœªæå–åˆ°'}")
        logger.info(f"  - æ—¶é—´çº¿äº‹ä»¶: {len(timeline)}ä¸ª")
        
        # ç¬¬ 2 æ­¥ï¼šä½¿ç”¨ LLM è·å–è¯­ä¹‰ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æè¿°ã€åˆ†ç±»ï¼‰
        logger.info("ğŸ¤– æ­¥éª¤2: ä½¿ç”¨LLMæå–è¯­ä¹‰ä¿¡æ¯...")
        llm_result = await self._parse_with_llm(extracted_text)
        
        title = llm_result.get('title', 'æ´»åŠ¨')
        description = llm_result.get('description', '')
        category_str = llm_result.get('category', 'activity')
        llm_tags = llm_result.get('tags', [])
        
        logger.info(f"  - æ ‡é¢˜: {title}")
        logger.info(f"  - åˆ†ç±»: {category_str}")
        logger.info(f"  - LLMæ ‡ç­¾: {llm_tags}")
        
        # LLMå¯èƒ½è¿”å›æ›´å¥½çš„timeline
        if 'events' in llm_result and llm_result['events']:
            llm_timeline = llm_result['events'][0].get('timeline', [])
            if llm_timeline and len(llm_timeline) > len(timeline):
                logger.info(f"  - ä½¿ç”¨LLMæå–çš„æ—¶é—´çº¿ ({len(llm_timeline)}ä¸ªäº‹ä»¶)")
                timeline = [TimelineEvent(
                    deadline=t['deadline'],
                    comment=t['comment']
                ) for t in llm_timeline]
        
        # ç¡®ä¿ category æ˜¯æœ‰æ•ˆçš„ Enum å€¼
        try:
            category = ActivityCategory(category_str)
        except (ValueError, KeyError):
            category = ActivityCategory.ACTIVITY
        
        # ç¬¬ 3 æ­¥ï¼šè§„åˆ™æå–æ ‡ç­¾ä½œä¸ºè¡¥å……
        rule_tags = self.extract_tags(title, extracted_text)
        
        # åˆå¹¶æ ‡ç­¾ï¼šLLMä¼˜å…ˆï¼Œè§„åˆ™è¡¥å……
        tags = []
        if llm_tags:
            tags.extend(llm_tags)
        tags.extend([t for t in rule_tags if t not in tags])
        tags = tags[:5]  # æœ€å¤š5ä¸ª
        
        logger.info(f"  - æœ€ç»ˆæ ‡ç­¾: {tags}")
        
        # å¦‚æœ LLM æ²¡æœ‰æå–æè¿°ï¼Œä½¿ç”¨è§„åˆ™æå–
        if not description:
            description = self.extract_description(extracted_text)
            logger.info("  - ä½¿ç”¨è§„åˆ™æå–çš„æè¿°")
        
        # æ„å»ºäº‹ä»¶
        event = ActivityEvent(
            year=datetime.now().year,
            id=self._generate_id(title),
            link=source_url or '',
            date=date_str or '',
            place=place or '',
            timeline=timeline
        )
        
        # æ„å»ºæ´»åŠ¨
        activity = ParsedActivity(
            title=title,
            description=description,
            category=category,
            tags=tags,
            events=[event]
        )
        
        logger.info(f"âœ… è§£æå®Œæˆ: {activity.title}")
        
        return activity
    
    async def _parse_with_llm(self, text: str) -> Dict:
        """ä½¿ç”¨ LLM è§£æ - ç›´æ¥ä½¿ç”¨github_models_parserçš„å®Œæ•´è§£æ"""
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.llm:
            logger.warning("âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œè¿”å›é»˜è®¤å€¼")
            return {"title": "æ´»åŠ¨", "description": "", "category": "activity", "tags": []}
        
        try:
            # ç›´æ¥è°ƒç”¨ GitHubModelsParser.parse()ï¼Œå®ƒä¼šè¿”å›å®Œæ•´çš„ç»“æ„
            logger.info("ğŸ¤– è°ƒç”¨GitHub Models API...")
            response = await self.llm.parse(text)
            
            if response and 'title' in response:
                logger.info(f"âœ… LLMè§£ææˆåŠŸ: {response.get('title', 'Unknown')}")
                return response
            elif 'error' in response:
                logger.warning(f"âš ï¸ LLMè¿”å›é”™è¯¯: {response['error']}")
                return {"title": "æ´»åŠ¨", "description": "", "category": "activity", "tags": []}
            else:
                logger.warning("âš ï¸ LLMè¿”å›ç©ºç»“æœ")
                return {"title": "æ´»åŠ¨", "description": "", "category": "activity", "tags": []}
        except Exception as e:
            logger.error(f"âŒ LLMè§£æå¤±è´¥: {e}")
            return {"title": "æ´»åŠ¨", "description": "", "category": "activity", "tags": []}
            return {"title": "æ´»åŠ¨", "description": "", "category": "activity"}
    
    def _generate_id(self, title: str) -> str:
        """ç”Ÿæˆæ´»åŠ¨ ID"""
        import hashlib
        hash_obj = hashlib.md5(title.encode())
        return hash_obj.hexdigest()[:8]
